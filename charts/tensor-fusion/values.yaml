# Default values for tensor-fusion.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


# This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""
namespaceOverride: ""

#This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}

initialGpuNodeLabelSelector: "nvidia.com/gpu.present=true"

controller:
  # This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
  replicaCount: 1

  # This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
  image:
    repository: tensorfusion/tensor-fusion-operator
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"
  # This is for setting Kubernetes Annotations to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ 
  
  vectorAgentImage: docker.io/timberio/vector:latest-alpine

  podAnnotations: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchExpressions:
              - key: tensor-fusion.ai/component
                operator: In
                values:
                  - operator

  livenessProbe:
    httpGet:
      path: /healthz
      port: 8081
    initialDelaySeconds: 15
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 5
  readinessProbe:
    httpGet:
      path: /readyz
      port: 8081
    initialDelaySeconds: 5
    periodSeconds: 15
    timeoutSeconds: 5
    failureThreshold: 2
  resources:
    requests:
      memory: 256Mi
      cpu: 50m
    limits:
      memory: 2Gi
      cpu: 2000m

  admissionWebhooks:
    failurePolicy: Fail
    secretName: tensor-fusion-webhook-secret
    patch:
      image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.0
greptime:
  isCloud: false
  host: greptimedb-standalone.greptimedb.svc.cluster.local
  port: 4002
  db: public
  installStandalone: true
  image:
    repository: docker.io/greptime/greptimedb
    tag: latest
  resources:
    requests:
      memory: 512Mi
      cpu: 100m
    limits:
      memory: 2Gi
      cpu: 200m

# greptime:
#   isCloud: true
#   host: your-instance.us-west-2.aws.greptime.cloud
#   user: "dummy"
#   db: "public"
#   password: "dummy"
#   port: 5001

agent:
  enrollToken: "token-from-cloud"
  agentId: 'org-from-cloud:env'
  cloudEndpoint: "wss://app.tensor-fusion.ai"
  
  image:
    repository: tensorfusion/tensor-fusion-agent
    tag: "latest"
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 4000m
      memory: 2Gi

# Only needed if your pool is running in Provisioned mode, and the cloud vendor doesn't support IRSA or any serviceAccount like zero-credential Auth approaches
cloudVendorCredentials:
  accessKey: "dummy"
  secretKey: "dummy"

alert:
  enabled: true
  image:
    repository: quay.io/prometheus/alertmanager
    tag: v0.28.1
  replicaCount: 1
  resources:
    requests:
      memory: 256Mi
      cpu: 50m
    limits:
      memory: 1Gi
      cpu: 1500m
  alertManagerConfig:
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
    templates:
    - /etc/alertmanager/*.tmpl

# KV structure config for other global configs
dynamicConfig:
  # retention period for metrics data
  metricsTTL: 30d

  # alert rules
  alertRules:    
    # Worker TFlops throttled alert
    - name: WorkerTFlopsThrottled
      query: |
        SELECT workload, worker, uuid, node_name, MAX(compute_throttled_cnt)-MIN(compute_throttled_cnt) as throttled_increase
        FROM tf_worker_usage
        WHERE {{ .Conditions }}
        GROUP BY workload, worker, uuid, node_name
        HAVING throttled_increase > {{ .Threshold }}
      threshold: 0
      evaluationInterval: 15s
      consecutiveCount: 3
      severity: P1
      summary: "Worker TFlops Throttled"
      description: "Worker {{ .worker }} from Node {{ .node_name }} is using more than {{ .Threshold }}% of its TFlops limit"
      alertTargetInstance: "{{ .worker }}-{{ .uuid }}"
      runbookURL: "https://tensor-fusion.ai/guide/troubleshooting/handbook"
    
    # Worker VRAM switching too frequent alert
    - name: WorkerVRAMSwitchCountIncreasing
      query: |
        SELECT workload, worker, uuid, node_name, MAX(vram_resumed_cnt)-MIN(vram_resumed_cnt) as switch_increase
        FROM tf_worker_usage 
        WHERE {{ .Conditions }}
        GROUP BY workload, worker, uuid, node_name
        HAVING switch_increase > {{ .Threshold }}
      threshold: 0
      evaluationInterval: 2m
      consecutiveCount: 1
      severity: P1
      summary: "Worker VRAM Switch Count Increasing"
      description: "Worker {{ .worker }} from Node {{ .node_name }} has switched VRAM {{ .switch_increase }} times in last 2 minutes, GPU may be too hot"
      alertTargetInstance: "{{ .worker }}-{{ .uuid }}"
      runbookURL: "https://tensor-fusion.ai/guide/troubleshooting/handbook"
    
    # Worker can not scale up/scheduled alert
    - name: WorkerAllocationFailed
      query: |
        SELECT pool, (MAX(total_allocation_fail_cnt) - MIN(total_allocation_fail_cnt)) as failure_increase,
        FROM tf_system_metrics
        WHERE {{ .Conditions }}
        GROUP BY pool
        HAVING failure_increase > {{ .Threshold }}
      threshold: 0
      evaluationInterval: 30s
      consecutiveCount: 1
      severity: P1
      summary: "Worker allocation failed for GPU Pool {{ .pool }}"
      description: "Worker allocation failed, {{ .failure_increase }} times in last 30 seconds for GPU Pool {{ .pool }}"
      alertTargetInstance: "{{ .pool }}"
      runbookURL: "https://tensor-fusion.ai/guide/troubleshooting/handbook"
    
    # Single GPU Alerts
    
    # GPU VRAM Full Alert
    - name: GPUVRAMFull
      query: |
        SELECT
          node_name,
          pool,
          uuid,
          avg(memory_percentage) AS memory_used
        FROM tf_gpu_usage
        WHERE memory_percentage > {{ .Threshold }} AND {{ .Conditions }}
        GROUP BY node_name, pool, uuid
      threshold: 97
      evaluationInterval: 30s
      consecutiveCount: 2
      severity: P1
      summary: "GPU VRAM Full, used {{ .memory_used }}% on {{ .node_name }} {{ .uuid }}"
      alertTargetInstance: "{{ .uuid }}"
      description: "GPU {{ .uuid }} on Node {{ .node_name }} in Pool {{ .pool }} has VRAM usage above {{ .Threshold }}% for 2 consecutive 30s, average usage: {{ .memory_used }}%"
    
    # GPU TFlops Full Alert
    - name: GPUTFlopsFull
      query: |
        SELECT
          node_name,
          pool,
          uuid,
          avg(compute_percentage) AS compute_used
        FROM tf_gpu_usage
        WHERE compute_percentage > {{ .Threshold }} AND {{ .Conditions }}
        GROUP BY node_name, pool, uuid
      threshold: 97
      evaluationInterval: 30s
      consecutiveCount: 4
      severity: P1
      summary: "GPU TFlops Full, used {{ .compute_used }}% on {{ .node_name }} {{ .uuid }}"
      alertTargetInstance: "{{ .uuid }}"
      description: "GPU {{ .uuid }} on Node {{ .node_name }} in Pool {{ .pool }} has TFlops usage above {{ .Threshold }}% for 4 consecutive 30s, average usage: {{ .compute_used }}%"
    
    # GPU Temperature alert
    - name: GPUTemperatureHigh
      query: |
        SELECT
          node_name,
          pool,
          uuid,
          avg(temperature) AS avg_temperature
        FROM tf_gpu_usage
        WHERE temperature > {{ .Threshold }} AND {{ .Conditions }}
        GROUP BY node_name, pool, uuid
      threshold: 90
      evaluationInterval: 30s
      consecutiveCount: 3
      severity: P1
      summary: "GPU Temperature High, {{ .avg_temperature }}°C on {{ .node_name }} {{ .uuid }}"
      alertTargetInstance: "{{ .uuid }}"
      description: "GPU {{ .uuid }} from Node {{ .node_name }} has temperature above {{ .Threshold }}°C, Average temperature: {{ .avg_temperature }}, GPU Pool: {{ .pool }}"
      runbookURL: "https://tensor-fusion.ai/guide/troubleshooting/handbook"
    
    # GPU Pool Alerts
    
    # Node TFlops allocation alert
    - name: NodeTFlopsAllocationCritical
      query: | 
        SELECT node_name, pool, (100 - avg(allocated_tflops_percent)) as tflops_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY node_name, pool
        HAVING tflops_available < {{ .Threshold }}
      threshold: 5
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P0
      summary: "Available TFlops below threshold, remaining {{ .tflops_available }}% for {{ .node_name }}"
      description: "Node {{ .node_name }} in Pool {{ .pool }} has available TFlops below {{ .Threshold }}%"
      alertTargetInstance: "{{ .node_name }}"
    
    - name: NodeTFlopsAllocationWarning
      query: | 
        SELECT node_name, pool, (100 - avg(allocated_tflops_percent)) as tflops_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY node_name, pool
        HAVING tflops_available < {{ .Threshold }}
      threshold: 10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Node available TFlops below threshold, remaining {{ .tflops_available }}% for {{ .node_name }}"
      description: "Node {{ .node_name }} in Pool {{ .pool }} has available TFlops below {{ .Threshold }}%"
      alertTargetInstance: "{{ .node_name }}"
    
    # Pool TFlops allocation alert - Total
    - name: PoolTotalTFlopsAllocationCritical
      query: |
        SELECT pool, (100 - avg(allocated_tflops_percent)) as tflops_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY pool
        HAVING tflops_available < {{ .Threshold }}
      threshold: 5
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P0
      summary: "Pool available TFlops below threshold, remaining {{ .tflops_available }}%"
      description: "Pool {{ .pool }} has available TFlops below {{ .Threshold }}%"
      alertTargetInstance: "{{ .pool }}"
    
    - name: PoolTotalTFlopsAllocationWarning
      query: |
        SELECT pool, (100 - avg(allocated_tflops_percent)) as tflops_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY pool
        HAVING tflops_available < {{ .Threshold }}
      threshold: 10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool available TFlops below threshold, remaining {{ .tflops_available }}%"
      description: "Pool {{ .pool }} has available TFlops below {{ .Threshold }}%"
      alertTargetInstance: "{{ .pool }}"
    
    # Node VRAM allocation alert
    - name: NodeVRAMAllocationCritical
      query: |
        SELECT node_name, pool, (100 - avg(allocated_vram_percent)) as vram_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY node_name, pool
        HAVING vram_available < {{ .Threshold }}
      threshold: 5
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Node available VRAM below threshold, remaining {{ .vram_available }}% for {{ .node_name }}"
      description: "Node {{ .node_name }} in Pool {{ .pool }} has available VRAM below {{ .Threshold }}%"
      alertTargetInstance: "{{ .node_name }}"

    - name: NodeVRAMAllocationWarning
      query: |
        SELECT node_name, pool, (100 - avg(allocated_vram_percent)) as vram_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY node_name, pool
        HAVING vram_available < {{ .Threshold }}
      threshold: 10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Node available VRAM below threshold, remaining {{ .vram_available }}% for {{ .node_name }}"
      description: "Node {{ .node_name }} in Pool {{ .pool }} has available VRAM below {{ .Threshold }}%"
      alertTargetInstance: "{{ .node_name }}"
    
    # Pool VRAM allocation alert
    - name: PoolVRAMAllocationWarning
      query: |
        SELECT pool, (100 - avg(allocated_vram_percent)) as vram_available
        FROM tf_node_resources
        WHERE {{ .Conditions }}
        GROUP BY pool
        HAVING vram_available < {{ .Threshold }}
      threshold: 10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool available VRAM below threshold, remaining {{ .vram_available }}% for {{ .pool }}"
      description: "Pool {{ .pool }} has available VRAM below {{ .Threshold }}%"
      alertTargetInstance: "{{ .pool }}"
    
    # Empty or Idle GPU Alert
    - name: EmptyGPU
      query: |
        SELECT DISTINCT node_name 
        FROM tf_node_resources 
        WHERE {{ .Conditions }} AND node_name NOT IN (
            SELECT DISTINCT node_name 
            FROM tf_worker_usage 
            WHERE {{ .Conditions }}
        )
      threshold: 0
      evaluationInterval: 5m
      consecutiveCount: 2
      severity: P2
      summary: "Empty GPU without any workload, Node {{ .node_name }}"
      description: "GPU Node {{ .node_name }} has no workload running, should be decommissioned"
      alertTargetInstance: "{{ .node_name }}"
    
    - name: IdleGPU
      query: |
        SELECT node_name, pool, uuid, avg(compute_percentage) as compute, avg(memory_percentage) vram
        FROM tf_gpu_usage
        WHERE {{ .Conditions }}
        GROUP BY node_name, pool, uuid
        HAVING compute < 1 and vram < {{ .Threshold }};
      threshold: 5
      evaluationInterval: 10m
      consecutiveCount: 3
      severity: P2
      summary: "Idle GPU found: {{ .uuid }} on Node {{ .node_name }}"
      description: "GPU {{ .uuid }} on Node {{ .node_name }} in Pool {{ .pool }} has been idle for 3 consecutive 10m, compute: {{ .compute }}, vram: {{ .vram }}"
      alertTargetInstance: "{{ .uuid }}"
